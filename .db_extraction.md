# Data Extraction

Data Collection
- [x] Download PubMed and PMC
  
Initial Filtering
- [x] gather all abstracts containing:
  - a url &rarr; mostly "http(s)://..." sometimes just ".org" or ".gov"
  - OR terms hinting at a database &rarr; "database(s)"/"dataset"/...

ML setup (Training Classifier)
- [ ] chose the most 5k probable database hits -> manually label them 
  - [ ] figure out if its a db or sth else (AI -> some kind of BERT?)
  - [ ] fetch all nessesary data (name, papers(oldest -> newest)) from pubmed 
- [ ] fetch additional data from pubmed + PMC (organisms, field, type of data(structure, RNAseq, genomics, metabolomics, pathways, ...), corresponding github repos)
- [ ] check if the url/db still exists (pfam as good example)
  - get headers from the site (py equivalent of $curl -I), check if it still exists and is not moved

- scape the landing pge to see what the site is about?

